{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3aeb4b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.ticker import FixedLocator, FixedFormatter, FuncFormatter, MultipleLocator\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "\n",
    "from src.ForecastModel.data.models import DataModelCV\n",
    "from src.ForecastModel.utils.losses import loss_peak_mse\n",
    "from src.ForecastModel.utils.metrics import (evaluate_multistep,\n",
    "                                             calculate_rms, calculate_bias, \n",
    "                                             calculate_bias_flv, calculate_bias_fhv,\n",
    "                                             calculate_bias, calculate_nse, calculate_kge,\n",
    "                                            )\n",
    "from src.ForecastModel.utils.postprocessing import ModelHandler, dt\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": False,\n",
    "    'font.size'   : 8,\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d595bf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOT_PATH          = r\"plots\"\n",
    "DATA_PATH          = r\"src\\data\\Dataset.csv\"\n",
    "CROSS_INDICES_PATH = r\"src\\data\\indices\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b57dc291",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"arima\": ModelHandler(\"ARIMA\",\n",
    "                r\"rst\\ARIMA\",\n",
    "                is_final_model = True,\n",
    "                is_external_model = True,\n",
    "                color = \"black\",\n",
    "                ls = \"--\",\n",
    "                  ),\n",
    "     \"elstm\": ModelHandler(\"eLSTM\",\n",
    "                   r\"rst\\eLSTM\",\n",
    "                   is_final_model = True,\n",
    "                   color = '#984ea3',\n",
    "                   ls = \"-\",\n",
    "                 ),\n",
    "     \"pbhm-hlstm\": ModelHandler(\"PBHM-HLSTM\",\n",
    "              r\"rst\\PBHM-HLSTM\",\n",
    "               is_final_model = True,\n",
    "               color = \"#e41a1c\",\n",
    "              ls = \"-\",\n",
    "              )\n",
    "     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd760cf1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arima\n",
      "elstm\n",
      "dictonary loaded\n",
      "35/35 [==============================] - 2s 7ms/step\n",
      "35/35 [==============================] - 1s 7ms/step\n",
      "35/35 [==============================] - 1s 8ms/step\n",
      "35/35 [==============================] - 1s 14ms/step\n",
      "35/35 [==============================] - 1s 12ms/step\n",
      "pbhm-hlstm\n",
      "dictonary loaded\n",
      "35/35 [==============================] - 1s 15ms/step\n",
      "35/35 [==============================] - 1s 12ms/step\n",
      "35/35 [==============================] - 1s 10ms/step\n",
      "35/35 [==============================] - 1s 11ms/step\n",
      "35/35 [==============================] - 1s 10ms/step\n"
     ]
    }
   ],
   "source": [
    "# define metrics to evaluate\n",
    "eval_metrics = {\n",
    "        \"fhv\" : calculate_bias_fhv,\n",
    "        \"flv\" : calculate_bias_flv,\n",
    "        \"bias\" : calculate_bias,\n",
    "        \"nse\" : calculate_nse,\n",
    "        \"kge\" : calculate_kge,\n",
    "    }\n",
    "\n",
    "colors = [\"red\", \"blue\", \"green\", \"magenta\", \"gray\", \"black\"]\n",
    "idx = -10\n",
    "for n, key in enumerate(models.keys()):\n",
    "    idx += 10\n",
    "    print(key)\n",
    "\n",
    "    metrics = {\n",
    "        \"valid\": {}, \n",
    "        \"test\" : {},\n",
    "    }\n",
    "    for k, item in eval_metrics.items():\n",
    "        metrics[\"test\"][k] = []\n",
    "\n",
    "    if models[key].is_external_model:\n",
    "        overlap_length = 0\n",
    "        hindcast_length = 96\n",
    "    else:\n",
    "        # load datamodel\n",
    "        dm = DataModelCV(DATA_PATH,\n",
    "           target_name       = models[key].target_name,\n",
    "           hincast_features  = models[key].feat_hindcast,\n",
    "           forecast_features = models[key].feat_forecast,\n",
    "         )\n",
    "        \n",
    "        # load trial data\n",
    "        with open(os.path.join(models[key].hp_path, \"trial.json\")) as f:\n",
    "            trial = json.load(f)\n",
    "\n",
    "        hindcast_length = trial['hyperparameters']['values']['hindcast_length']\n",
    "        try:\n",
    "            overlap_length = trial['hyperparameters']['values']['osc_length']\n",
    "        except:\n",
    "            overlap_length = 0 \n",
    "        \n",
    "        dm.main(os.path.join(CROSS_INDICES_PATH, f\"cross_indices_{hindcast_length}.pkl\"))\n",
    "\n",
    "    res = np.array([])\n",
    "    yp_all = np.array([])\n",
    "    for n_fold in range(5):\n",
    "        year = 2013 + n_fold\n",
    "        if models[key].is_external_model:\n",
    "            # load external which come already with observations\n",
    "            ext_df = pd.read_pickle(os.path.join(models[key].hp_path, f\"forecast_{year}.pkl\"))\n",
    "\n",
    "            # get external model observations\n",
    "            y = np.expand_dims(ext_df.filter(like=\"obs\").values, axis=2) # fix \n",
    "\n",
    "            # get external model predictions\n",
    "            yp = ext_df.filter(like=\"fc\").values\n",
    "                        \n",
    "        else:\n",
    "            # load dataset\n",
    "            X, y  = dm.getDataSet(dm.cross_sets[n_fold][\"test\"], scale=True) \n",
    "\n",
    "            \n",
    "            #if os.path.exists(os.path.join(models[key].hp_path, f\"forecast_{year}.pkl\")):\n",
    "             #   yp = pd.read_pickle(os.path.join(models[key].hp_path, f\"forecast_{year}.pkl\")).values\n",
    "            #else:\n",
    "                # load model\n",
    "            tf.keras.backend.clear_session()\n",
    "            model  = tf.keras.models.load_model(os.path.join(models[key].hp_path, f\"model_fold_{n_fold:d}.keras\"))                          \n",
    "\n",
    "            # model prediction\n",
    "            yp = model.predict(X, batch_size=1000)\n",
    "\n",
    "        for k, item in eval_metrics.items():\n",
    "            metrics[\"test\"][k].append(evaluate_multistep(y, yp, item))\n",
    "\n",
    "    with open(os.path.join(models[key].hp_path, f\"metrics_eval.txt\"), \"w+\") as f:\n",
    "        json.dump(metrics, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2fd540-6b1d-47df-8b27-dd30902bf3a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
